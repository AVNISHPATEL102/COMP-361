# -*- coding: utf-8 -*-
"""Chapter4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tgteacher/numerical-methods/blob/master/notebooks/Chapter4.ipynb

# Chapter 4: Roots of (non-linear) Equations

## 1. Introduction

Goal of this chapter:

**Find the solutions of f(x)=0, where the function f is given.**

The solutions of this equation are called the *roots* of the equation or the *zeroes* of the function f.

The function $f$ might be defined as:
1. A mathematical statement, for instance: f(x) = log(x)
2. A computer algorithm.

$f$ might have:
1. A single zero, for instance: f(x) = log(x)
2. Multiple zeroes: f(x) = $x^2$
3. An infinite number of zeroes: f(x) = cos(x)
4. No zero: f(x) = cos(x) + 2

We focus only on *real* zeroes.

The methods presented in this chapter:
* Are meant to find *a single* root.
* Are iterative: they start from an estimate of the root and refine it.
* Usually require prior *bracketing* of the root: determine its lower and upper bounds.
* Assume that $f$ is *continuous* in the bracket.

Extensions can be derived to lift some of these conditions.

### Illustration

Bracketing and iterative root-finding method.
"""

from utils import *
ch4_intro_plot()

"""## 2. Incremental Search

Main idea
* If $f(x_1)$ and $f(x_2)$ have opposite signs, then the root is in [$x_1$, $x_2$].
* $\Rightarrow$ Evaluate $f$ at interval $\Delta x$ and look for a change of sign.

### Illustration

$f(x_1)$ and $f(x_2)$ have different signs $\Rightarrow$ the root is in [$x_1$, $x_2$].
"""

incremental_search_plot()

"""Be careful, sign inversions might also occur when function is not continuous!

### Example

Using *incremental search* with $\Delta x=0.1$, find the zero of $f(x)=x^2-2$ in the interval [1, 2].

### Implementation
"""

from numpy import sign
def root_find_incremental(f, xmin, xmax, delta_x):
    '''
    f is the function for which we will find a zero
    xmin and xmax define the bracket
    delta_x is the step used in incremental search
    Returns (a+b)/2, where the root is in [a, b] and |b-a| < delta_x
    '''
    a = xmin
    b = a + delta_x
    fa = f(a)
    fb = f(b)
    estimates = [] # we will store the successive estimates in this list, for visualization purposes
    while(b <= xmax):
        root_estimate = (a+b)/2
        estimates.append(root_estimate)
        if sign(fa) != sign(fb):
            return root_estimate, estimates
        a = b
        b += delta_x
        fa = fb
        fb = f(b)
    raise Exception("Could not find root in bracket")

"""### Example"""

from math import tan
f = tan
x, estimates = root_find_incremental(f, 2, 4, 0.01)
print(x, "({} estimates)".format(len(estimates)))

plot_estimates(estimates)

"""### Issues
* Accurate estimation of the root requires a small $\Delta x$ value
* Number of evaluations of $f$ is $\frac{x_{\mathrm{max}}-x_{\mathrm{min}}}{\Delta x}$ (worst case)

## 3. Method of Bisection

The bisection method *halves* the interval until it becomes smaller than $\Delta x$.

Main idea:
* If there is a root in [$x_1$, $x_2$], then f($x_1$) and f($x_2$) have opposite signs (as in incremental search).
* Define $x_3=\frac{x_1+x_2}{2}$. If f($x_1$) and f($x_3$) have opposite signs, then the root is in [$x_1$, $x_3$]; otherwise, it's in [$x_2$, $x_3$].

### Illustration
"""

bisection_init()

bisection_iter1()

bisection_iter2()

"""### Example

Using *bisection*, find the zero of $f(x)=x^2-2$ in the interval [1, 2].

### Implementation
"""

def root_find_bisection(f, xmin, xmax, delta_x):
    '''
    f is the function for which we will find a zero
    xmin and xmax define the bracket
    delta_x is the desired accuracy
    Returns (a+b)/2, where the root is in [a, b] and |b-a| <= delta_x
    '''
    
    a = xmin
    b = xmax
    fa = f(a)
    fb = f(b)
    if sign(fa) == sign(fb):
        raise Exception("Root is not bracketed") # Root is not bracketed
    estimates = [] # we will store the successive estimates in this list, for visualization purposes
    while(b-a > delta_x):
        # At this point, fa and fb always have different signs
        c = (a+b)/2
        estimates.append(c)
        fc = f(c)
        if sign(fc) == sign(fa):
            # The root must be in [c, b]
            a = c
            fa = fc
        else:
            b = c
            fb = fc
    estimates.append((a+b)/2)
    return (a+b)/2, estimates

"""### Example"""

f = tan
x, estimates = root_find_bisection(f, 2, 4, 0.01)
print(x, "({} estimates)".format(len(estimates)))

plot_estimates(estimates)

"""## 4. False Position Method

The False Position Method (a.k.a. *regula falsi*) uses a linear interpolation of $f$ between the two bounds of the bracket.

### Illustration
"""

plot_false_position()

"""### Formulation

Let [a, b] be the interval in which the zero has been bracketed.

Let $y = \alpha x + \beta$ be the straight line interpolating $f$ at a and b.

We have:
$$
\alpha = \frac{f(b)-f(a)}{b-a} \quad \beta = \frac{bf(a)-af(b)}{b-a}
$$

The method estimates the zero to be $c$ such that:
$$
\alpha c + \beta = 0
$$
which gives:
$$
c = \frac{af(b)-bf(a)}{f(b)-f(a)}
$$

Then, if f(a) and f(c) have the same sign (resp. f(b) and f(c) have the same sign), a (resp. b) is replaced by c and the method iterates.

### Example

Using the *false position* method, find the zero of $f(x)=x^2-2$ in the interval [1, 2].

### Implementation
"""

from numpy import sign

def false_position(f, a, b, delta_x, max_iter=1000):
    '''
    f is the function for which we will find a zero
    a and b define the bracket
    delta_x is the desired accuracy
    Returns ci such that |ci-c_{i-1}| < delta_x
    '''
    fa = f(a)
    fb = f(b)
    if sign(fa) == sign(fb):
        raise Exception("Root hasn't been bracketed")
    estimates = []
    for i in range(max_iter):
        c = (a*fb-b*fa)/(fb-fa)
        estimates.append(c)
        fc = f(c)
        if sign(fc) == sign(fa):
            a = c
            fa = fc
        else:
            b = c
            fb = fc
        if len(estimates) >=2 and abs(estimates[-1] - estimates[-2]) <= delta_x:
            break
    return c, estimates

from math import tan
x, estimates = false_position(tan, 2, 4, 0.01)
print(x, "({} estimates)".format(len(estimates)))

"""## 5. Newton-Raphson Method

This method is simple and fast, but it requires that the derivative of the function can be computed. This method approximates f by the straight line tangent to the curve at $x_i$:
"""

plot_newton_raphson_1()

plot_newton_raphson_2()

"""Graphically, the slope of the tangent to $f$ at $x_i$ (orange line) is:
$$
s = \frac{f(x_i)}{x_i-x_{i+1}}
$$
And by definition of $f'$, we also have $s = f'(x_i)$.

Thus:
$$
f'(x_i) = \frac{f(x_i)}{x_i-x_{i+1}}
$$
which gives:
$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)} \quad \textbf{(1)}
$$

### Example

Use the *Newton-Raphson* method to determine successive estimates of a zero of $f(x)=x^2-2$, starting with an estimate of $5$.

### Implementation
"""

def newton_raphson(f, diff, init_x, tol, max_iter=1000):
    '''
    f is the function for which a zero is seeked
    diff is the derivative of the function
    init_x is the initial estimate
    tol is the tolerance (accuracy) of the solution
    max_iter is the desired maximal number of iterations
    '''
    x = init_x
    estimates = []
    for i in range(max_iter): # we will break out of the loop when we find the root
        delta_x = -f(x)/diff(x)
        x = x + delta_x
        estimates.append(x)
        if abs(delta_x) <= tol:
            return x, estimates
    raise Exception("Unable to find a root")

"""### Example"""

from math import tan
def diff_tan(x):
    return 1 + tan(x)**2
x, estimates = newton_raphson(tan, diff_tan, 3, 0.01)
print(x, "({} estimates)".format(len(estimates)))

"""### Other tests (failed)

##### Null derivative
"""

def f1(x):
    return x**2-2

def diff1(x):
    return 2*x

newton_raphson(f1, diff1, 0, 10E-5)

"""What happened:"""

graph = newton_plot(f1, diff1, -2, 2, -4, 4, 0)

"""$\Rightarrow$ the method assumes that **$f'$ is not zero at and around the root**.

##### Initialization far from the root
"""

def f2(x):
    from math import exp, sin
    return exp(-0.1*x)*sin(x*3)+0.5

def diff2(x):
    from math import exp, sin, cos
    return -0.1*exp(-0.1*x)*sin(x*3)+exp(-0.1*x)*3*cos(3*x)

plot_function(f2)

newton_raphson(f2, diff2, 12, 10E-5)

graph = newton_plot(f2, diff2, 0, 15, -0.25, 1.5, 11.5, False)
graph.show()

"""$\Rightarrow$ initialization has to be sufficiently close to the root.

### A safer implementation

The following implementation falls back on bisection when:
* $f'(x)=0$
* The root estimate goes outside of the bracket
"""

def newton_raphson_safe(f, diff, tol, a, b, max_iter=1000):
    fa = f(a)
    fb = f(b)
    if sign(fa) == sign(fb):
        raise Exception("Root is not bracketed")
    x = (a+b)/2 # initial guess for Newton-Raphson
    for i in range(max_iter):
        # Try a Newton-Raphson iteration
        try:
            diff_x = diff(x)
            if diff_x == 0:
                raise NewtonRaphsonError("Derivative is 0")
            delta_x = -f(x)/diff(x)
            x += delta_x
            if x > b or x < a:
                raise NewtonRaphsonError("Estimate is going out of bound")
        except NewtonRaphsonError as e:
            # Fall back on bisection
            delta_x = (b-a)/2
            x = a + delta_x

        # Check for convergence
        if abs(delta_x) <= tol:
            return x
    
        # Tighten the bracket
        fx = f(x)
        if sign(fa) == sign(fx):
            a = x
            fa = fx
        else:
            b = x
            fb = fx
    raise NewtonRaphsonError("Cannot find root")
            
class NewtonRaphsonError(Exception):
    pass

x = newton_raphson_safe(f2, diff2, 10E-5, 0, 1.8)
print(x)

x = newton_raphson_safe(f1, diff1, 10E-5, -1, 2)
print(x)

"""### Recap on iteration and accuracy

Accuracy is set as a parameter $\delta_x$. 

|               | Use of $\delta_x$    | Iterates until
|----------------|------------------------|----------------------------|
| Incremental    | Step                   | Sign inversion is observed or estimate is out of bound|
| Bisection      | --                     | $b-a < \delta_x$           |
| False position | --                     | $|x_{n+1}-x_n| \leq \delta_x$ or max number of iterations reached|
| Newton-Raphson | --                     | $|x_{n+1}-x_n| \leq \delta_x$ or max number of iterations reached|

## 6. Systems of Equations

We now consider a system of $n$ non-linnear equations written as:
$$
\textbf{f}(\textbf{x})=\textbf{0}
$$

where $\textbf{f}$ is a vector of functions of $n$ variables:
$$
f_1(x_1, x_2, \ldots, x_n) = 0 \\
f_2(x_1, x_2, \ldots, x_n) = 0 \\
\ldots\\
f_n(x_1, x_2, \ldots, x_n) = 0 \\
$$

* There is no reliable method for bracketting the solution vector $\textbf{x}$. 
* The simplest and most effective resolution method is the Newton-Raphson one.
* The other methods (incremental, bisection, false position) don't generalize well to systems of equations.

### [Newton](https://en.wikipedia.org/wiki/Isaac_Newton)-[Raphson](https://en.wikipedia.org/wiki/Joseph_Raphson) method

#### Formulation

A linear (aka first-order) approximation of $\textbf{f}$ in the neighborhood of $\textbf{x}$ gives:
$$
\textbf{f}(\textbf{x}+\Delta \textbf{x}) = \textbf{f}(\textbf{x}) + \textbf{J}(\textbf{x})\Delta \textbf{x} \quad \textbf{(2)}
$$

where $\textbf{J}(\textbf{x})$ is the [Jacobian](https://en.wikipedia.org/wiki/Carl_Gustav_Jacob_Jacobi) matrix of $\textbf{f}$ at $\textbf{x}$.

The Jacobian matrix is an $n \times n$ matrix $\textbf{J}$ defined as follows:
$$
J_{ij} = \frac{\partial f_i}{\partial x_j}
$$
or:
$$
\textbf{J} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n}\\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n}\\
\ldots\\
\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \ldots & \frac{\partial f_n}{\partial x_n}\\
\end{bmatrix}
$$

In scalar form, equation $\textbf{(2)}$ is written as follows:
$$
f_i(\textbf{x}+\Delta \textbf{x}) = f_i(\textbf{x}) + \sum_{j=1}^n \frac{\partial f_i}{\partial x_j} \Delta x_j
$$

Setting $\textbf{f}(\textbf{x}+\Delta \textbf{x})$ to 0 (since it is the new approximation of the solution), we obtain:
$$
\textbf{J}(\textbf{x})\Delta \textbf{x} = -\textbf{f}(\textbf{x}) \quad \textbf{(2)}
$$
Note the similarity with equation $\textbf{(1)}$ above (Newton-Raphson iteration for a single equation).

#### Implementation

The algorithm is written as follows:
1. Estimate the solution vector $\textbf{x}$
2. Do until $||\Delta \textbf{x} || < \epsilon$:
   * Compute $\textbf{J}(\textbf{x})$ using Equation $\textbf{(3)}$ below
   * Solve $\textbf{J}(\textbf{x})\Delta \textbf{x} = -\textbf{f}(\textbf{x})$ in $\Delta\textbf{x}$
   * Let $\textbf{x} \leftarrow \textbf{x} + \Delta \textbf{x}$

Partial derivatives of $\textbf{f}$ at $\textbf{x}$ are computed as follows:
$$
\frac{\partial f_i}{\partial x_j}(\textbf{x}) \approx \frac{f_i(\textbf{x}+h\textbf{e}_j)-f_i(\textbf{x})}{h} \quad \textbf{(3)}
$$
where $h$ is a small increment of $x_j$ and $e_j$ is a unit vector in direction $j$:
$$\textbf{e}_j=
\begin{bmatrix}
0 \\
0 \\
\ldots\\
1\\
\ldots \\
0 \\
0
\end{bmatrix}
$$
"""

from numpy import zeros

def jacobian(f, x):
    '''
    Returns the Jacobian matrix of f taken in x J(x)
    '''
    n = len(x)
    jac = zeros((n, n))
    h = 10E-4
    fx = f(x)
    # go through the columns of J
    for j in range(n):
        # compute x + h ej
        old_xj = x[j]
        x[j] += h
        # update the Jacobian matrix (eq 3)
        # Now x is x + h*ej
        jac[:, j] = (f(x)-fx) / h 
        # restore x[j]
        x[j] = old_xj
    return jac

from numpy.linalg import solve
from numpy import sqrt

def newton_raphson_system(f, init_x, epsilon=10E-4, max_iterations=100):
    '''
    Return a solution of f(x)=0 by Newton-Raphson method.
    init_x is the initial guess of the solution
    '''
    x = init_x
    for i in range(max_iterations):
        J = jacobian(f, x)
        delta_x = solve(J, -f(x)) # we could also use our functions from Chapter 2!
        x = x + delta_x
        if sqrt(sum(delta_x**2)) <= epsilon:
            print("Converged in {} iterations".format(i))
            return x
    raise Exception("Could not find root!")

"""### Example"""

from numpy import array

def f(x):
    return array([x[0]**2 + x[1]**2 -3, x[0]*x[1]-1])

solution = newton_raphson_system(f, [0.5, 1.5])
print(solution)

"""### Example (4.9 in the textbook):"""

from numpy import array
from math import sin, log

def f(xvec):
    x = xvec[0]
    y = xvec[1]
    z = xvec[2]
    return array([
        sin(x) + y**2 + log(z) - 7,
        3*x + 2**y - z**3 + 1,
        x + y + z - 5
    ])

x = array([1,1,1.0])
solution = newton_raphson_system(f, x)
print(solution)

"""## 7. Zeroes of polynomials

### Introduction

A polynomial of degree $n$ has the form:
$$
P_n(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
$$

It has *exactly* $n$ zeroes, some of which may be complex.

We assume that $a_i \in \mathbb{R}$, but the same methods work when $a_i \in \mathbb{C}$.

### Evaluation of polynomials

The following algorithm computes $P(x)$ "from left to right".

It is straightforward, but it leads to many unnecessary operations:
"""

def eval_p_naive(a, x):
    '''
    This function returns P(x)
    The coefficients of the polynomial are in array a
    '''
    result = a[0]
    n = len(a)
    for i in range(1, n):
        result += a[i]*x**i
    return result

eval_p_naive(array([0, 1, 1]), 4)

"""Total number of multiplications is $\sum_{i=1}^{n-1}\ln(i)=\ln((n-1)!)$

Rewriting the polynomial as follows leads to a better algorithm:

$$
P_n(x) = a_0 + x \left( a_1 + x \left( a_2 + x \left (\ldots + xa_n \right) \right) \right)
$$

which results in the following sequence:
$$
P_0(x) = a_n\\
P_i(x) = a_{n-i} + x P_{i-1}(x) \quad 1 \leq i \leq n
$$
"""

def eval_p(a, x):
    '''
    Returns P(x) where the coefficients of P are in a
    '''
    n = len(a)
    p = a[n-1]
    for i in range(2, n+1):
        p = a[n-i] + x*p
    return p

eval_p([0, 1, 1], 4)

"""Total number of multiplications is n-1.

### Evaluation of derivatives

The relation between $P_n$ and $P_{n-1}$ can also be used to compute derivatives of $P$:

$$
P_0'(x) = 0 \\
P_i'(x) = P_{i-1}(x) + xP'_{i-1}(x) \quad 1 \leq i \leq n
$$

and:

$$
P''_0(x) = 0 \\
P_i''(x) = 2P'_{i-1}(x) + xP''_{i-1}(x) \quad 1 \leq i \leq n
$$
"""

def eval_p_dp_ddp(a, x):
    '''
    Returns P(x), P'(x) and P''(x) where the coefficients of P are in a
    '''
    n = len(a)
    p = a[n-1]
    dp = 0
    ddp = 0
    for i in range(2, n+1):
        # careful with the order!
        ddp = 2*dp + x*ddp
        dp = p + x*dp
        p = a[n-i] + x*p
    return p, dp, ddp

eval_p_dp_ddp([0, 1, 1], 4)

eval_p([1, 0, 1], 3)

"""### Deflation of polynomials

After a root $r$ of $P_n$ has been found, $P_n$ can be factorized as follows:
$$
P_n(x) = (x-r)P_{n-1}(x)
$$

This procedure is called *deflation*. This is useful because:
* The remaining zeroes of $P_n$ are the zeroes of $P_{n-1}$.
* $P_{n-1}$ is of degree $n-1$.

#### Determining $P_{n-1}$

$P_{n-1}$ can be written as follows:
$$
P_{n-1}(x) = b_0 + b_1x + b_2x^2 + \ldots + b_{n-1}x^{n-1} 
$$

Therefore:
$$
P_n(x) = (x-r)P_{n-1}(x) = b_0(x-r) + b_1x(x-r) + b_2x^2(x-r) + \ldots + b_{n-1}x^{n-1}(x-r)
$$

By identification with the coefficients of $P_n$:
$$
b_{n-1} = a_n \\
b_{n-2} = a_{n-1} + rb_{n-1} \\
\ldots \\
b_0 = a_1 + rb_1
$$

This leads to [Horner](https://en.wikipedia.org/wiki/William_George_Horner)'s deflation algorithm:
"""

from numpy import zeros
def deflate(a, r):
    '''
    Returns an array with the coefficients of P_{n-1} where P_n(x) = (x-r)P_{n-1}(x)
    The coefficients of Pn are in a
    r is a root of Pn
    '''
    n = len(a) - 1 # n is the degree of Pn. There are n+1 coefficients in Pn
    assert(n>=1)
    b = zeros(n, dtype=complex)
    b[n-1] = a[n]
    for i in range(n-2, -1, -1): # decreasing index
        b[i] = a[i+1] + r*b[i+1]
    return b

"""#### Example (4.10 in textbook)"""

from numpy import array
deflate(array([12, -2, -48, -10, 3.0]), 6)

"""### Laguerre's method for polynomial root-finding

Laguerre's method is an iterative method to find polynomial roots.

#### Principle

*It works in the general case*, but to explain the formula here, we consider a polynomial in the following form:
$$
P_n(x)=(x-r)(x-q)^{n-1} \quad \textbf{(a)}
$$

Our problem is now to determine $r$ given the polynomial in Equation $\textbf{(a)}$ in the form:
$$
P_n(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
$$
Note that q is also unknown.

We will express $r$ as a function of $P_n$, $P'_n$ and $P''_n$.

By differentiation of $\textbf{(a)}$, we get:

$$
P'_n(x) = (x-q)^{n-1} + (n-1)(x-r)(x-q)^{n-2} \\
        = P_n(x)\left(\frac{1}{x-r} + \frac{n-1}{x-q} \right)
$$

Which gives:  
$$
\frac{P'_n(x)}{P_n(x)} = \frac{1}{x-r} + \frac{n-1}{x-q} \quad \textbf{(b)}
$$

Another differentiation gives:
$$
\frac{P''_n(x)P_n(x)-P'_n(x)P'_n(x)}{P_n^2(x)}=\frac{-1}{(x-r)^2}+\frac{(1-n)}{(x-q)^2}
$$

i.e.:
    
$$
\frac{P_n''(x)}{P_n(x)}-\left( \frac{P_n'(x)}{P_n(x)}\right)^2 = - \frac{1}{(x-r)^2} - \frac{n-1}{(x-q)^2} \quad \textbf{(c)}
$$

Let's set the following notations:
$$
G(x) = \frac{P_n'(x)}{P_n(x)} \quad \textbf{(d)}\\
\mathrm{and} \\
H(x) = G(x)^2 - \frac{P_n''(x)}{P_n(x)} \quad \textbf{(e)}
$$

We have:
$$
G(x) = \frac{1}{x-r} + \frac{n-1}{x-q} \quad \mathrm{from\ Eq. \textbf{(b)}}\\
H(x) = \frac{1}{(x-r)^2} + \frac{n-1}{(x-q)^2} \quad \mathrm{from\ Eq. \textbf{(c)}}
$$

We can solve these equations in $r$ and $q$ using the following change of variables:
$$
R(x) = \frac{1}{x-r} \\
Q(x) = \frac{1}{x-q}
$$
We obtain:
$$
x-r=\frac{n}{G(x) \pm \sqrt{(n-1)\left(nH(x)-G(x)^2\right)}} \quad \textbf{(f)}
$$

#### Algorithm

The resulting root-finding algorithm is as follows:

Repeat until $|P(x)|\leq \epsilon$ or $|x-r| \leq \epsilon$:
   * Evaluate $P(x)$, $P'(x)$ and $P''(x)$
   * Evaluate $H(x)$ and $G(x)$ from equations $\textbf{(d)}$ and $\textbf{(e)}$ 
   * Determine $\Delta x = x-r$ from equation $\textbf{(f)}$ by choosing *the solution with the largest magnitude in the denominator*
   * Replace $x$ by $r$

#### Implementation
"""

import cmath 
from random import random
def laguerre(a, x=random(), epsilon=1E-12, max_iters=100):
    '''
    Returns one root of the polynomial in a using Laguerre's method
    '''
    n = len(a) - 1
    for i in range(max_iters):
        p, dp, ddp = eval_p_dp_ddp(a, x)
        if abs(p) < epsilon:
            print("Converged in {} iterations".format(i))
            return x
        g = dp/p # eq d
        h = g*g - ddp/p # eq e
        sqr = cmath.sqrt((n-1)*(n*h-g*g)) # note import from cmath so that sqrt is defined on R
        if abs(g+sqr) > abs(g-sqr):
            delta = n / (g+sqr) # eq f
        else:
            delta = n / (g-sqr) # eq f
        x = x - delta # delta = x- r -> x <- r
        if abs(delta) < epsilon:
            print("Converged in {} iterations".format(i))
            return x
    raise Exception("Did not find a root")

from numpy import array
a = array([-1, 1, 1], dtype=complex)
laguerre(a)

"""#### Finding all the roots of a polynomial

Laguerre's method finds *one* root of the polynomial. We can use polynomial deflation to find *all* the roots of a polynomial:
"""

def roots(a):
    '''
    Returns all the roots of the polynomials with coefficients in a
    '''
    n = len(a) - 1
    roots = zeros(n, dtype=complex) # there are exactly n roots
    for i in range(n):
        r = laguerre(a)
        a = deflate(a, r)
        roots[i] = r
    return roots

"""#### Example (4.12 in textbook)"""

# Let's do it!

from numpy import array
a = array([-250, 155, -9, -5, 1], dtype=float)

roots(a)

